{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordnet"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
   "metadata": {},
=======
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 47 political instability keywords.\n"
     ]
    }
   ],
   "source": [
    "ids_political_instability_bingAI = [\"Anarchy\", \"Authoritarianism\", \"Civil unrest\", \"Corruption\", \"Coup\", \"Dictatorship\",\n",
    "                                    \"Dissent\", \"Economic crisis\", \"Failed state\", \"Fascism\", \"Genocide\", \"Human rights abuses\",\n",
    "                                    \"Insurgency\", \"Military rule\", \"Oppression\", \"Political violence\", \"Poverty\", \"Protest\", \"Rebellion\",\n",
    "                                    \"Regime change\", \"Revolution\", \"Sectarianism\", \"Autocracy\", \"Bureaucracy\", \"Civil war\", \"Conflict\",\n",
    "                                    \"Coup d'état\", \"Democracy deficit\", \"Despotism\", \"Economic collapse\", \"Ethnic conflict\",\n",
    "                                    \"Failed government\", \"Humanitarian crisis\", \"Inequality\", \"Injustice\", \"Kleptocracy\",\n",
    "                                    \"Military dictatorship\", \"Oligarchy\", \"Political corruption\", \"Political repression\", \"Poverty trap\"]\n",
    "                            \n",
    "ids_political_instability_copilot = [\"Repression\", \"Revolt\", \"Secession\", \"Social conflict\", \"State collapse\", \"State failure\"]\n",
    "ids_political_instability = list(set(ids_political_instability_copilot + ids_political_instability_bingAI))\n",
    "\n",
    "ids_political_instability = [x.lower() for x in ids_political_instability]\n",
    "\n",
    "print(\"We have {} political instability keywords.\".format(len(ids_political_instability)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Geopolitical Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 58 geopolitical factors keywords.\n"
     ]
    }
   ],
   "source": [
    "ids_geopolitical_factors_copilot = [\"Border dispute\", \"Border tension\", \"Border war\", \"Cold war\", \"Colonialism\", \"Decolonization\",\n",
    "                                    \"Disputed territory\", \"Expansionism\", \"Foreign intervention\", \"Foreign policy\", \"Geopolitics\",\n",
    "                                    \"Great power rivalry\", \"Hegemony\", \"Imperialism\", \"International relations\", \"Isolationism\",\n",
    "                                    \"Military occupation\", \"Military presence\", \"Military threat\", \"Nationalism\", \"Neocolonialism\",\n",
    "                                    \"Non-alignment\", \"Proxy war\", \"Regional conflict\", \"Regional rivalry\", \"Sphere of influence\",\n",
    "                                    \"Superpower rivalry\", \"Territorial dispute\", \"Territorial integrity\", \"Treaty\", \"War\"\n",
    "                                    \"War of aggression\", \"War of independence\", \"War of liberation\", \"War of succession\",\n",
    "                                    \"War of unification\", \"War on terror\", \"Warfare\", \"World war\"]\n",
    "                            \n",
    "ids_geopolitical_factors_bingAI = [\"Alliances\", \"Balance of power\", \"Border disputes\", \"Cold War\", \"Colonization\", \"Diplomacy\",\n",
    "                                    \"Economic sanctions\", \"Embargoes\", \"Foreign aid\", \"Geopolitical rivalry\", \"Globalization\",\n",
    "                                    \"Hegemony\", \"Imperialism\", \"International law\", \"Interventionism\", \"Military bases\", \n",
    "                                    \"National security\", \"Nuclear proliferation\", \"Proxy wars\", \"Regional integration\",\n",
    "                                    \"Resource competition\", \"Strategic alliances\"]\n",
    "\n",
    "ids_geopolitical_factors = list(set(ids_geopolitical_factors_copilot + ids_geopolitical_factors_bingAI))\n",
    "ids_geopolitical_factors = [x.lower() for x in ids_geopolitical_factors]\n",
    "\n",
    "print(\"We have {} geopolitical factors keywords.\".format(len(ids_geopolitical_factors)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) Currency fluctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 44 currency fluctuations keywords.\n"
     ]
    }
   ],
   "source": [
    "ids_currency_fluctuations_copilot = [\"Currency crisis\", \"Currency devaluation\", \"Currency fluctuations\", \"Currency war\", \"Devaluation\",\n",
    "                                     \"Spot contract\", \"Trade deficit\", \"Trade surplus\", \"Trade war\", \"Trade-off\", \"Trade-weighted index\",\n",
    "                                     \"Trading bloc\", \"Trading partner\", \"Trading volume\", \"Volatility\", \"World trade\",\n",
    "                                     \"World trade organization\", \"Worldwide trade\", \"Yield curve\", \"Zero-sum game\", \"Zero-sum outcome\",\n",
    "                                     \"Zero-sum relationship\", \"Zero-sum scenario\", \"Zero-sum situation\", \"Zero-sum strategy\"]\n",
    "\n",
    "ids_currency_fluctuations_bingAI = [\"Appreciation\", \"Arbitrage\", \"Balance of payments\", \"Bear market\", \"Bull market\", \"Capital flight\",\n",
    "                                    \"Central bank intervention\", \"Currency crisis\", \"Currency peg\", \"Devaluation\", \"Exchange rate\",\n",
    "                                    \"Fiscal policy\", \"Foreign exchange market\", \"Forward contract\", \"Hedging\", \"Inflation\",\n",
    "                                    \"Interest rates\", \"Monetary policy\", \"Purchasing power parity\", \"Quantitative easing\", \"Speculation\"]\n",
    "\n",
    "ids_currency_fluctuations = list(set(ids_currency_fluctuations_copilot + ids_currency_fluctuations_bingAI))\n",
    "ids_currency_fluctuations = [x.lower() for x in ids_currency_fluctuations]\n",
    "\n",
    "print(\"We have {} currency fluctuations keywords.\".format(len(ids_currency_fluctuations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4) Investment demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 40 investment demand keywords.\n"
     ]
    }
   ],
   "source": [
    "ids_investment_demand_bingAI = [\"Asset allocation\", \"Bear market\", \"Bond\", \"Bull market\", \"Capital gains\", \"Capital loss\",\n",
    "                                \"Commodities\", \"Compound interest\", \"Corporate bonds\", \"Credit rating\", \"Debt securities\",\n",
    "                                \"Derivatives\", \"Dividend yield\", \"Equity securities\", \"Exchange-traded funds (ETFs)\",\n",
    "                                \"Fixed income securities\", \"Foreign exchange market (Forex)\", \"Futures contracts\",\n",
    "                                \"Hedge funds\", \"Inflation-indexed bonds\", \"Interest rate risk\", \"Investment-grade bonds\",\n",
    "                                \"Junk bonds\", \"Market capitalization\", \"Municipal bonds\", \"Mutual funds\",\n",
    "                                \"Options contracts\", \"Real estate investment trusts (REITs)\", \"Risk tolerance\", \"Securities lending\"]\n",
    "                                \n",
    "ids_investment_demand_copilot = [\"Stocks\", \"Taxable bonds\", \"Treasury bonds\", \"Treasury inflation-protected securities (TIPS)\",\n",
    "                                 \"Treasury notes\", \"Treasury securities\", \"savings bonds\", \"Treasury bills\", \"Treasury bonds\",\n",
    "                                 \"Treasury notes\", \"Treasury securities\", \"Treasury STRIPS\", \"Zero-coupon bonds\"]\n",
    "\n",
    "ids_investment_demand = list(set(ids_investment_demand_copilot + ids_investment_demand_bingAI))\n",
    "ids_investment_demand = [x.lower() for x in ids_investment_demand]\n",
    "\n",
    "print(\"We have {} investment demand keywords.\".format(len(ids_investment_demand)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5) Supply and demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 32 supply and demand keywords.\n"
     ]
    }
   ],
   "source": [
    "ids_supply_and_demand_bingAI = [\"Allocation\", \"Availability\", \"Balance\", \"Competition\", \"Consumer\", \"Demand\", \"Elasticity\", \"Equilibrium\",\n",
    "                                \"Excess\", \"Export\", \"Import\", \"Inflation\", \"Market\", \"Monopoly\", \"Need\", \"Price\", \"Production\",\n",
    "                                \"Quantity\", \"Scarcity\", \"Shortage\", \"Stock\"]\n",
    "\n",
    "ids_supply_and_demand_copilot = [\"Aggregate demand\", \"Aggregate supply\", \"Demand curve\", \"Demand schedule\", \"Demand shock\",\n",
    "                                  \"Supply\", \"Surplus\", \"Trade\", \"Value\", \"Want\", \"Yield\"]\n",
    "\n",
    "ids_supply_and_demand = list(set(ids_supply_and_demand_copilot + ids_supply_and_demand_bingAI))\n",
    "ids_supply_and_demand = [x.lower() for x in ids_supply_and_demand]\n",
    "\n",
    "print(\"We have {} supply and demand keywords.\".format(len(ids_supply_and_demand)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6) Industrial demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 36 industrial demand keywords.\n"
     ]
    }
   ],
   "source": [
    "ids_industrial_demand_bingAI = [\"Automation\", \"Capacity\", \"Competition\", \"Consumer demand\", \"Cost\", \"Efficiency\", \"Elasticity\",\n",
    "                                \"Employment\", \"Energy consumption\", \"Equipment\", \"Export demand\", \"Import demand\", \"Innovation\",\n",
    "                                \"Labor force\", \"Logistics\", \"Manufacturing\", \"Market demand\", \"Materials\", \"Output\",\n",
    "                                \"Productivity\", \"Quality control\", \"Raw materials\", \"Supply chain\"]\n",
    "\n",
    "ids_industrial_demand_copilot = [\"Aggregate demand\", \"Aggregate supply\", \"Demand curve\", \"Demand schedule\", \"Demand shock\",\n",
    "                                 \"Technology\", \"Transportation\", \"Value\", \"Want\", \"Yield\", \"Supply\", \"Surplus\", \"Trade\"]\n",
    "\n",
    "ids_industrial_demand = list(set(ids_industrial_demand_copilot + ids_industrial_demand_bingAI))\n",
    "ids_industrial_demand = [x.lower() for x in ids_industrial_demand]\n",
    "\n",
    "print(\"We have {} industrial demand keywords.\".format(len(ids_industrial_demand)))\n",
    "                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7) Natural disasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 36 natural disasters keywords.\n"
     ]
    }
   ],
   "source": [
    "ids_natural_disasters_bingAI = [\"Avalanche\", \"Blizzard\", \"Catastrophe\", \"Cyclone\", \"Drought\", \"Earthquake\", \"Erosion\", \"Flood\",\n",
    "                                \"Forest fire\", \"Hailstorm\", \"Heat wave\", \"Hurricane\", \"Landslide\", \"Lightning\", \"Magma\", \"Natural wildfire\",\n",
    "                                \"Nimbus\", \"Rainstorm\", \"Sandstorm\", \"Seismic\", \"Thunderstorm\", \"Tremor\", \"Tsunami\", \"Twister\", \"Volcano eruption\"]\n",
    "\n",
    "ids_natural_disasters_copilot = [\"Tornado\", \"Wildfire\", \"Winter storm\", \"Winter weather\", \"Winter weather advisory\", \"Winter weather warning\",\n",
    "                                 \"Winter storm\", \"Tropical cyclone\", \"Tropical storm\", \"Typhoon\", \"Volcano\", \"Volcanic eruption\"]\n",
    "\n",
    "ids_natural_disasters = list(set(ids_natural_disasters_copilot + ids_natural_disasters_bingAI))\n",
    "ids_natural_disasters = [x.lower() for x in ids_natural_disasters]\n",
    "\n",
    "print(\"We have {} natural disasters keywords.\".format(len(ids_natural_disasters)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'political instability': ['authoritarianism',\n",
       "  'bureaucracy',\n",
       "  'civil unrest',\n",
       "  'political corruption',\n",
       "  'genocide',\n",
       "  'military rule',\n",
       "  'dictatorship',\n",
       "  'corruption',\n",
       "  \"coup d'état\",\n",
       "  'conflict',\n",
       "  'anarchy',\n",
       "  'ethnic conflict',\n",
       "  'repression',\n",
       "  'humanitarian crisis',\n",
       "  'human rights abuses',\n",
       "  'insurgency',\n",
       "  'protest',\n",
       "  'state collapse',\n",
       "  'regime change',\n",
       "  'oligarchy',\n",
       "  'poverty',\n",
       "  'failed state',\n",
       "  'revolt',\n",
       "  'military dictatorship',\n",
       "  'autocracy',\n",
       "  'secession',\n",
       "  'state failure',\n",
       "  'political repression',\n",
       "  'revolution',\n",
       "  'sectarianism',\n",
       "  'despotism',\n",
       "  'fascism',\n",
       "  'social conflict',\n",
       "  'political violence',\n",
       "  'economic crisis',\n",
       "  'democracy deficit',\n",
       "  'rebellion',\n",
       "  'oppression',\n",
       "  'coup',\n",
       "  'injustice',\n",
       "  'civil war',\n",
       "  'inequality',\n",
       "  'failed government',\n",
       "  'poverty trap',\n",
       "  'dissent',\n",
       "  'economic collapse',\n",
       "  'kleptocracy'],\n",
       " 'geopolitical factors': ['strategic alliances',\n",
       "  'balance of power',\n",
       "  'war on terror',\n",
       "  'economic sanctions',\n",
       "  'military presence',\n",
       "  'resource competition',\n",
       "  'hegemony',\n",
       "  'war of unification',\n",
       "  'proxy war',\n",
       "  'territorial integrity',\n",
       "  'foreign intervention',\n",
       "  'warfare',\n",
       "  'nationalism',\n",
       "  'colonization',\n",
       "  'nuclear proliferation',\n",
       "  'regional rivalry',\n",
       "  'military bases',\n",
       "  'geopolitics',\n",
       "  'border war',\n",
       "  'border tension',\n",
       "  'foreign policy',\n",
       "  'war of independence',\n",
       "  'war of liberation',\n",
       "  'world war',\n",
       "  'neocolonialism',\n",
       "  'regional integration',\n",
       "  'geopolitical rivalry',\n",
       "  'globalization',\n",
       "  'border disputes',\n",
       "  'military occupation',\n",
       "  'territorial dispute',\n",
       "  'non-alignment',\n",
       "  'interventionism',\n",
       "  'isolationism',\n",
       "  'treaty',\n",
       "  'cold war',\n",
       "  'proxy wars',\n",
       "  'great power rivalry',\n",
       "  'disputed territory',\n",
       "  'war of succession',\n",
       "  'warwar of aggression',\n",
       "  'foreign aid',\n",
       "  'sphere of influence',\n",
       "  'alliances',\n",
       "  'cold war',\n",
       "  'diplomacy',\n",
       "  'decolonization',\n",
       "  'national security',\n",
       "  'international relations',\n",
       "  'border dispute',\n",
       "  'military threat',\n",
       "  'superpower rivalry',\n",
       "  'imperialism',\n",
       "  'colonialism',\n",
       "  'expansionism',\n",
       "  'regional conflict',\n",
       "  'international law',\n",
       "  'embargoes'],\n",
       " 'currency fluctuations': ['trade war',\n",
       "  'zero-sum game',\n",
       "  'quantitative easing',\n",
       "  'speculation',\n",
       "  'currency war',\n",
       "  'spot contract',\n",
       "  'exchange rate',\n",
       "  'currency crisis',\n",
       "  'zero-sum strategy',\n",
       "  'fiscal policy',\n",
       "  'zero-sum relationship',\n",
       "  'trade-off',\n",
       "  'capital flight',\n",
       "  'world trade',\n",
       "  'zero-sum scenario',\n",
       "  'balance of payments',\n",
       "  'trade surplus',\n",
       "  'interest rates',\n",
       "  'currency devaluation',\n",
       "  'volatility',\n",
       "  'trade-weighted index',\n",
       "  'zero-sum outcome',\n",
       "  'trading volume',\n",
       "  'bear market',\n",
       "  'bull market',\n",
       "  'currency fluctuations',\n",
       "  'forward contract',\n",
       "  'devaluation',\n",
       "  'trading partner',\n",
       "  'yield curve',\n",
       "  'trading bloc',\n",
       "  'monetary policy',\n",
       "  'trade deficit',\n",
       "  'hedging',\n",
       "  'arbitrage',\n",
       "  'zero-sum situation',\n",
       "  'purchasing power parity',\n",
       "  'foreign exchange market',\n",
       "  'world trade organization',\n",
       "  'currency peg',\n",
       "  'central bank intervention',\n",
       "  'inflation',\n",
       "  'worldwide trade',\n",
       "  'appreciation'],\n",
       " 'investment demand': ['foreign exchange market (forex)',\n",
       "  'treasury notes',\n",
       "  'stocks',\n",
       "  'debt securities',\n",
       "  'capital loss',\n",
       "  'inflation-indexed bonds',\n",
       "  'asset allocation',\n",
       "  'taxable bonds',\n",
       "  'real estate investment trusts (reits)',\n",
       "  'securities lending',\n",
       "  'mutual funds',\n",
       "  'treasury strips',\n",
       "  'municipal bonds',\n",
       "  'savings bonds',\n",
       "  'treasury bills',\n",
       "  'zero-coupon bonds',\n",
       "  'dividend yield',\n",
       "  'treasury inflation-protected securities (tips)',\n",
       "  'equity securities',\n",
       "  'bear market',\n",
       "  'bull market',\n",
       "  'capital gains',\n",
       "  'treasury bonds',\n",
       "  'risk tolerance',\n",
       "  'treasury securities',\n",
       "  'futures contracts',\n",
       "  'investment-grade bonds',\n",
       "  'exchange-traded funds (etfs)',\n",
       "  'hedge funds',\n",
       "  'fixed income securities',\n",
       "  'market capitalization',\n",
       "  'options contracts',\n",
       "  'credit rating',\n",
       "  'derivatives',\n",
       "  'corporate bonds',\n",
       "  'junk bonds',\n",
       "  'bond',\n",
       "  'interest rate risk',\n",
       "  'commodities',\n",
       "  'compound interest'],\n",
       " 'supply and demand': ['consumer',\n",
       "  'allocation',\n",
       "  'trade',\n",
       "  'want',\n",
       "  'aggregate supply',\n",
       "  'elasticity',\n",
       "  'availability',\n",
       "  'competition',\n",
       "  'stock',\n",
       "  'demand curve',\n",
       "  'production',\n",
       "  'import',\n",
       "  'equilibrium',\n",
       "  'supply',\n",
       "  'balance',\n",
       "  'export',\n",
       "  'need',\n",
       "  'scarcity',\n",
       "  'shortage',\n",
       "  'demand',\n",
       "  'quantity',\n",
       "  'surplus',\n",
       "  'price',\n",
       "  'market',\n",
       "  'monopoly',\n",
       "  'yield',\n",
       "  'demand shock',\n",
       "  'inflation',\n",
       "  'value',\n",
       "  'excess',\n",
       "  'aggregate demand',\n",
       "  'demand schedule'],\n",
       " 'industrial demand': ['productivity',\n",
       "  'energy consumption',\n",
       "  'transportation',\n",
       "  'trade',\n",
       "  'market demand',\n",
       "  'want',\n",
       "  'aggregate supply',\n",
       "  'elasticity',\n",
       "  'competition',\n",
       "  'supply chain',\n",
       "  'demand curve',\n",
       "  'automation',\n",
       "  'consumer demand',\n",
       "  'equipment',\n",
       "  'logistics',\n",
       "  'export demand',\n",
       "  'supply',\n",
       "  'technology',\n",
       "  'raw materials',\n",
       "  'innovation',\n",
       "  'efficiency',\n",
       "  'surplus',\n",
       "  'quality control',\n",
       "  'cost',\n",
       "  'labor force',\n",
       "  'capacity',\n",
       "  'import demand',\n",
       "  'manufacturing',\n",
       "  'materials',\n",
       "  'yield',\n",
       "  'output',\n",
       "  'demand shock',\n",
       "  'employment',\n",
       "  'value',\n",
       "  'aggregate demand',\n",
       "  'demand schedule'],\n",
       " 'natural disasters': ['hurricane',\n",
       "  'volcano eruption',\n",
       "  'blizzard',\n",
       "  'typhoon',\n",
       "  'tornado',\n",
       "  'volcanic eruption',\n",
       "  'cyclone',\n",
       "  'natural wildfire',\n",
       "  'heat wave',\n",
       "  'tropical cyclone',\n",
       "  'catastrophe',\n",
       "  'landslide',\n",
       "  'avalanche',\n",
       "  'winter storm',\n",
       "  'hailstorm',\n",
       "  'earthquake',\n",
       "  'twister',\n",
       "  'volcano',\n",
       "  'winter weather',\n",
       "  'forest fire',\n",
       "  'winter weather warning',\n",
       "  'nimbus',\n",
       "  'thunderstorm',\n",
       "  'tremor',\n",
       "  'magma',\n",
       "  'tsunami',\n",
       "  'drought',\n",
       "  'erosion',\n",
       "  'lightning',\n",
       "  'rainstorm',\n",
       "  'wildfire',\n",
       "  'seismic',\n",
       "  'winter weather advisory',\n",
       "  'flood',\n",
       "  'sandstorm',\n",
       "  'tropical storm']}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = [\"political instability\", \"geopolitical factors\", \"currency fluctuations\", \"investment demand\", \"supply and demand\", \"industrial demand\", \"natural disasters\"]\n",
    "values = [ids_political_instability, ids_geopolitical_factors, ids_currency_fluctuations, ids_investment_demand, ids_supply_and_demand, ids_industrial_demand, ids_natural_disasters]\n",
    "dict_keywords = dict(zip(keys, values))\n",
    "dict_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:14:31.057481Z",
     "start_time": "2023-11-03T09:14:31.046588Z"
    }
   },
>>>>>>> 7993297b30a61b2506f9afb5d38f110cce33df25
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import string\n",
    "from collections import deque\n",
    "from typing import List, Union, Dict, Set, Tuple, Sequence\n",
    "\n",
    "# nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus.reader.wordnet import Synset\n",
    "from nltk import pos_tag\n",
    "# from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
   "metadata": {},
=======
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:14:31.550530Z",
     "start_time": "2023-11-03T09:14:31.200314Z"
    }
   },
>>>>>>> 7993297b30a61b2506f9afb5d38f110cce33df25
   "outputs": [],
   "source": [
    "# load data\n",
    "df_prepro_data = pd.read_csv('preprocessed_data.csv')\n",
    "df_prepro_data = df_prepro_data[df_prepro_data['title'].notna()]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
   "metadata": {},
=======
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:14:31.556867Z",
     "start_time": "2023-11-03T09:14:31.552362Z"
    }
   },
>>>>>>> 7993297b30a61b2506f9afb5d38f110cce33df25
   "outputs": [],
   "source": [
    "# function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Stemming\n",
    "    # stemmer = PorterStemmer()\n",
    "\n",
    "    # Removing stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "    # Normalization (converting to lower case and removing punctuation)\n",
    "    normalized_tokens = [re.sub(r'[^\\w\\s]', '', word.lower()) for word in filtered_tokens]\n",
    "    normalized_tokens = [word for word in normalized_tokens if word]  # Remove empty strings\n",
    "    normalized_tokens = [word for word in normalized_tokens if not word.isdigit()] # Remove numbers\n",
    "    normalized_tokens = [word for word in normalized_tokens if not len(word) == 1] # Remove single characters\n",
    "    normalized_tokens = [word for word in normalized_tokens if pos_tag([word])[0][1] in ['NN', 'NNS', 'NNP', 'NNPS']] # Remove non-nouns\n",
    "    normalized_tokens = [word for word in normalized_tokens if word not in string.punctuation] # Remove punctuation\n",
    "    normalized_tokens = list(filter(lambda x: not re.search(r'\\d', x), normalized_tokens)) # Remove tokens with numbers\n",
    "    # normalized_tokens = list({stemmer.stem(token) for token in normalized_tokens}) # Stemming\n",
    "\n",
    "    return normalized_tokens"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
   "metadata": {},
=======
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:14:31.563268Z",
     "start_time": "2023-11-03T09:14:31.554243Z"
    }
   },
>>>>>>> 7993297b30a61b2506f9afb5d38f110cce33df25
   "outputs": [],
   "source": [
    "# Build-up functions that calculate the similarity between two words \n",
    "\n",
    "def shortest_paths_to(start_syn: Synset) -> Dict[Synset, int]:\n",
    "    \"\"\"Compute the shortest distance to all nodes on paths to the root.\n",
    "    :param start_syn: synset to which we want to compute the shortest distances\n",
    "    :return: dict that matches all visited hypernyms to their distance to the input synset  \n",
    "    \"\"\" \n",
    "    # create set that keeps track of visited nodes\n",
    "    visited = set()\n",
    "    # create list which stores nodes (including distances) that need to be processed\n",
    "    queue = [(start_syn, 0)]\n",
    "    # create result dicitionary\n",
    "    distances = {}\n",
    "\n",
    "    # check nodes in the list queue as long it is not empty\n",
    "    while len(queue) > 0:\n",
    "        # remove nodes from front of list\n",
    "        syn, dist = queue.pop(0)\n",
    "        # check unvisited synsets\n",
    "        if syn in visited:\n",
    "            continue\n",
    "        visited.add(syn)\n",
    "        distances[syn] = dist\n",
    "        # loop over all direct hypernyms in list (input synset might be an instance)\n",
    "        for hyp in syn.hypernyms() + syn.instance_hypernyms():\n",
    "            # append it to queue (with dist+1) if not already looked at it before\n",
    "            if hyp not in visited or distances[hyp] > dist + syn.path_similarity(hyp):\n",
    "                queue.append((hyp, dist + syn.path_similarity(hyp)))\n",
    "                distances[hyp] = dist + syn.path_similarity(hyp)\n",
    "            # we only want to store the shortest distances\n",
    "            elif distances[hyp] > dist + syn.path_similarity(hyp):\n",
    "                distances[hyp] = dist + syn.path_similarity(hyp)\n",
    "    return distances\n",
    "\n",
    "def merge_paths(p1: Dict[Synset, int], p2: Dict[Synset, int]) -> Dict[Synset, int]:\n",
    "    \"\"\"Merge two paths keeping the shorter distance for synsets that appear more than once.\n",
    "    :param p1: first dict that maps synsets to their shortest distances\n",
    "    :param p2: second dict that maps synsets to their shortest distances\n",
    "    :return: merged dict\n",
    "    \"\"\"\n",
    "    # create resulting dictionary\n",
    "    merged = {}\n",
    "    # loop over all keys of both dictionaries\n",
    "    for synset in set(p1.keys()) | set(p2.keys()):\n",
    "        # for synsets in p1 and p2 we keep the shorter distance\n",
    "        if synset in p1 and synset in p2:\n",
    "            merged[synset] = min(p1[synset], p2[synset])\n",
    "        # distance of p1 if synset only appears in p1\n",
    "        elif synset in p1:\n",
    "            merged[synset] = p1[synset]\n",
    "        # distance of p2 if synset only appears in p2\n",
    "        else:\n",
    "            merged[synset] = p2[synset]\n",
    "            \n",
    "    return merged\n",
    "\n",
    "def all_hypernym_paths(word: str) -> Dict[Synset, int]:\n",
    "    \"\"\"Get all hypernyms of all synsets associated with the input word and compute the shortest distance leading there.\n",
    "    :param word: input word\n",
    "    :return: dict that matches all reachable hypernyms to their shortest distance \n",
    "    \"\"\"\n",
    "    # get synsets of input word\n",
    "    synsets = wn.synsets(word)\n",
    "    # create resulting dictionary\n",
    "    distances = {}\n",
    "\n",
    "    # using our functions of tasks a) and b)\n",
    "    for synset in synsets:\n",
    "        paths = shortest_paths_to(synset)\n",
    "        distances = merge_paths(distances, paths)\n",
    "\n",
    "    return distances\n",
    "\n",
    "def get_dist(w1 : str, w2 : str) -> float:\n",
    "    \"\"\"Compute the similarity between two input words in the WordNet hierarchy tree.\n",
    "    :param w1: first input word\n",
    "    :param w2: second input word\n",
    "    :return: word similarity\n",
    "    \"\"\"\n",
    "    # use function from c)\n",
    "    hyp1 = all_hypernym_paths(w1)\n",
    "    hyp2 = all_hypernym_paths(w2)\n",
    "    # filter on hypernyms that occur in both hypernym dictionaries\n",
    "    hyps = {k : hyp1[k] for k in hyp1 if k in hyp2}\n",
    "    # define d as shortest distance\n",
    "    if len(hyps) > 0:\n",
    "        d = min(hyps.values())\n",
    "        return d\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/paulkonig/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T09:14:49.921807Z",
     "start_time": "2023-11-03T09:14:48.168527Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:14:50.610909Z",
     "start_time": "2023-11-03T09:14:50.598695Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[['politics', 'instability'],\n ['geopolitics', 'factor'],\n ['currency', 'fluctuation'],\n ['investment', 'demand'],\n ['supply', 'demand'],\n ['industry', 'demand'],\n ['nature', 'disaster']]"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> 7993297b30a61b2506f9afb5d38f110cce33df25
   "source": [
    "# list of key features\n",
    "key_features = ['politics instability' , 'geopolitics factor' , 'currency fluctuation' ,\n",
    "                'investment demand' , 'supply and demand' , 'industry demand' , 'nature disaster']\n",
    "key_features_original = ['political instability' , 'geopolitical factors' , 'currency fluctuations' ,\n",
    "                        'investment demand' , 'supply and demand' , 'industrial demand' , 'natural disasters']\n",
    "key_features_weights = [[2/5, 3/5],[4/5, 1/5], [1/2, 1/2], [4/5, 1/5], [4/5, 1/5], [4/5, 1/5], [2/5, 3/5]]\n",
    "\n",
    "key_features_normalized = [preprocess_text(key) for key in key_features]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
   "metadata": {},
=======
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:25:35.600175Z",
     "start_time": "2023-11-03T09:25:35.583645Z"
    }
   },
>>>>>>> 7993297b30a61b2506f9afb5d38f110cce33df25
   "outputs": [],
   "source": [
    "# final classifier function\n",
    "def text_classifier(titles: List[str], key_features_tokens: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Classify the input title according to the input key features.\n",
    "    :param df: input list with titles\n",
    "    :param key_features: list of key features\n",
    "    :return: data frame with additional column that contains the classification\n",
    "    \"\"\"\n",
    "    df_classifier = np.zeros((len(titles), len(key_features_tokens)))\n",
    "    for i, title in enumerate(titles):\n",
    "        title_tokens = preprocess_text(title)\n",
    "        if len(title_tokens) == 0:\n",
    "            continue\n",
    "        for idx_kf, key_feature in enumerate(key_features_tokens):\n",
    "            avg_distance_tt_kf = 0\n",
    "            for idx_tt, title_token in enumerate(title_tokens):\n",
<<<<<<< HEAD
=======
    "                # average distance from 'political' to specific word in title\n",
    "                # avg_distance_tt_kf += sum([get_dist(key_feature_splitted, title_token) for key_feature_splitted in key_feature])/len(key_features)\n",
    "                avg_distance_tt_kf += sum([get_dist(key_feature_splitted, title_token) * key_features_weights[idx_kf][j] for j, key_feature_splitted in enumerate(key_feature)])/len(key_features)\n",
>>>>>>> 7993297b30a61b2506f9afb5d38f110cce33df25
    "\n",
    "                avg_distance_tt_kf += sum([get_dist(key_feature_splitted, title_token) * key_features_weights[idx_kf][j]\n",
    "                                           for j, key_feature_splitted in enumerate(key_feature)])/len(key_features)\n",
    "            df_classifier[i, idx_kf] = avg_distance_tt_kf / len(title_tokens)\n",
    "\n",
    "    return pd.DataFrame(data=df_classifier,  columns = key_features_original)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 21,
   "metadata": {},
=======
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T10:27:52.654213Z",
     "start_time": "2023-11-03T09:25:36.486060Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[36], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m classifications \u001B[38;5;241m=\u001B[39m \u001B[43mtext_classifier\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_prepro_data\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtitle\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey_features_normalized\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m classifications[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m df_prepro_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "Cell \u001B[0;32mIn[35], line 17\u001B[0m, in \u001B[0;36mtext_classifier\u001B[0;34m(titles, key_features_tokens)\u001B[0m\n\u001B[1;32m     13\u001B[0m         avg_distance_tt_kf \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     14\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m idx_tt, title_token \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(title_tokens):\n\u001B[1;32m     15\u001B[0m             \u001B[38;5;66;03m# average distance from 'political' to specific word in title\u001B[39;00m\n\u001B[1;32m     16\u001B[0m             \u001B[38;5;66;03m# avg_distance_tt_kf += sum([get_dist(key_feature_splitted, title_token) for key_feature_splitted in key_feature])/len(key_features)\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m             avg_distance_tt_kf \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(\u001B[43m[\u001B[49m\u001B[43mget_dist\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey_feature_splitted\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtitle_token\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mkey_features_weights\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx_kf\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43mj\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey_feature_splitted\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mkey_feature\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m)\u001B[38;5;241m/\u001B[39m\u001B[38;5;28mlen\u001B[39m(key_features)\n\u001B[1;32m     19\u001B[0m         df_classifier[i, idx_kf] \u001B[38;5;241m=\u001B[39m avg_distance_tt_kf \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(title_tokens)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pd\u001B[38;5;241m.\u001B[39mDataFrame(data\u001B[38;5;241m=\u001B[39mdf_classifier,  columns \u001B[38;5;241m=\u001B[39m key_features_original)\n",
      "Cell \u001B[0;32mIn[35], line 17\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     13\u001B[0m         avg_distance_tt_kf \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     14\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m idx_tt, title_token \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(title_tokens):\n\u001B[1;32m     15\u001B[0m             \u001B[38;5;66;03m# average distance from 'political' to specific word in title\u001B[39;00m\n\u001B[1;32m     16\u001B[0m             \u001B[38;5;66;03m# avg_distance_tt_kf += sum([get_dist(key_feature_splitted, title_token) for key_feature_splitted in key_feature])/len(key_features)\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m             avg_distance_tt_kf \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m([\u001B[43mget_dist\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey_feature_splitted\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtitle_token\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m*\u001B[39m key_features_weights[idx_kf][j] \u001B[38;5;28;01mfor\u001B[39;00m j, key_feature_splitted \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(key_feature)])\u001B[38;5;241m/\u001B[39m\u001B[38;5;28mlen\u001B[39m(key_features)\n\u001B[1;32m     19\u001B[0m         df_classifier[i, idx_kf] \u001B[38;5;241m=\u001B[39m avg_distance_tt_kf \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(title_tokens)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pd\u001B[38;5;241m.\u001B[39mDataFrame(data\u001B[38;5;241m=\u001B[39mdf_classifier,  columns \u001B[38;5;241m=\u001B[39m key_features_original)\n",
      "Cell \u001B[0;32mIn[27], line 80\u001B[0m, in \u001B[0;36mget_dist\u001B[0;34m(w1, w2)\u001B[0m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;66;03m# use function from c)\u001B[39;00m\n\u001B[1;32m     79\u001B[0m hyp1 \u001B[38;5;241m=\u001B[39m all_hypernym_paths(w1)\n\u001B[0;32m---> 80\u001B[0m hyp2 \u001B[38;5;241m=\u001B[39m \u001B[43mall_hypernym_paths\u001B[49m\u001B[43m(\u001B[49m\u001B[43mw2\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;66;03m# filter on hypernyms that occur in both hypernym dictionaries\u001B[39;00m\n\u001B[1;32m     82\u001B[0m hyps \u001B[38;5;241m=\u001B[39m {k : hyp1[k] \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m hyp1 \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m hyp2}\n",
      "Cell \u001B[0;32mIn[27], line 67\u001B[0m, in \u001B[0;36mall_hypernym_paths\u001B[0;34m(word)\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;66;03m# using our functions of tasks a) and b)\u001B[39;00m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m synset \u001B[38;5;129;01min\u001B[39;00m synsets:\n\u001B[0;32m---> 67\u001B[0m     paths \u001B[38;5;241m=\u001B[39m \u001B[43mshortest_paths_to\u001B[49m\u001B[43m(\u001B[49m\u001B[43msynset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     68\u001B[0m     distances \u001B[38;5;241m=\u001B[39m merge_paths(distances, paths)\n\u001B[1;32m     70\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m distances\n",
      "Cell \u001B[0;32mIn[27], line 26\u001B[0m, in \u001B[0;36mshortest_paths_to\u001B[0;34m(start_syn)\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m hyp \u001B[38;5;129;01min\u001B[39;00m syn\u001B[38;5;241m.\u001B[39mhypernyms() \u001B[38;5;241m+\u001B[39m syn\u001B[38;5;241m.\u001B[39minstance_hypernyms():\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;66;03m# append it to queue (with dist+1) if not already looked at it before\u001B[39;00m\n\u001B[1;32m     25\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m hyp \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m visited \u001B[38;5;129;01mor\u001B[39;00m distances[hyp] \u001B[38;5;241m>\u001B[39m dist \u001B[38;5;241m+\u001B[39m syn\u001B[38;5;241m.\u001B[39mpath_similarity(hyp):\n\u001B[0;32m---> 26\u001B[0m         queue\u001B[38;5;241m.\u001B[39mappend((hyp, dist \u001B[38;5;241m+\u001B[39m \u001B[43msyn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath_similarity\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhyp\u001B[49m\u001B[43m)\u001B[49m))\n\u001B[1;32m     27\u001B[0m         distances[hyp] \u001B[38;5;241m=\u001B[39m dist \u001B[38;5;241m+\u001B[39m syn\u001B[38;5;241m.\u001B[39mpath_similarity(hyp)\n\u001B[1;32m     28\u001B[0m     \u001B[38;5;66;03m# we only want to store the shortest distances\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/msg-datathon/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:871\u001B[0m, in \u001B[0;36mSynset.path_similarity\u001B[0;34m(self, other, verbose, simulate_root)\u001B[0m\n\u001B[1;32m    842\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpath_similarity\u001B[39m(\u001B[38;5;28mself\u001B[39m, other, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, simulate_root\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m    843\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    844\u001B[0m \u001B[38;5;124;03m    Path Distance Similarity:\u001B[39;00m\n\u001B[1;32m    845\u001B[0m \u001B[38;5;124;03m    Return a score denoting how similar two word senses are, based on the\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    866\u001B[0m \u001B[38;5;124;03m        itself.\u001B[39;00m\n\u001B[1;32m    867\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m    869\u001B[0m     distance \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshortest_path_distance(\n\u001B[1;32m    870\u001B[0m         other,\n\u001B[0;32m--> 871\u001B[0m         simulate_root\u001B[38;5;241m=\u001B[39msimulate_root \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_needs_root() \u001B[38;5;129;01mor\u001B[39;00m \u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_needs_root\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m),\n\u001B[1;32m    872\u001B[0m     )\n\u001B[1;32m    873\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m distance \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m distance \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    874\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/msg-datathon/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:479\u001B[0m, in \u001B[0;36mSynset._needs_root\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    478\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_needs_root\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 479\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pos \u001B[38;5;241m==\u001B[39m NOUN \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_wordnet_corpus_reader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_version\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1.6\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    480\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    481\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniconda3/envs/msg-datathon/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:1439\u001B[0m, in \u001B[0;36mWordNetCorpusReader.get_version\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1437\u001B[0m fh \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_file(ADJ)\n\u001B[1;32m   1438\u001B[0m fh\u001B[38;5;241m.\u001B[39mseek(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m-> 1439\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mline\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mfh\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m   1440\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmatch\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mre\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msearch\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mWord[nN]et (\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43md+|\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43md+\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43m.\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43md+) Copyright\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mline\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1441\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mmatch\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m:\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/msg-datathon/lib/python3.11/site-packages/nltk/data.py:1152\u001B[0m, in \u001B[0;36mSeekableUnicodeStreamReader.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1151\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__next__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m-> 1152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnext\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/msg-datathon/lib/python3.11/site-packages/nltk/data.py:1145\u001B[0m, in \u001B[0;36mSeekableUnicodeStreamReader.next\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1143\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m   1144\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return the next decoded line from the underlying stream.\"\"\"\u001B[39;00m\n\u001B[0;32m-> 1145\u001B[0m     line \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1146\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m line:\n\u001B[1;32m   1147\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m line\n",
      "File \u001B[0;32m~/miniconda3/envs/msg-datathon/lib/python3.11/site-packages/nltk/data.py:1100\u001B[0m, in \u001B[0;36mSeekableUnicodeStreamReader.readline\u001B[0;34m(self, size)\u001B[0m\n\u001B[1;32m   1098\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m   1099\u001B[0m     startpos \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream\u001B[38;5;241m.\u001B[39mtell() \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbytebuffer)\n\u001B[0;32m-> 1100\u001B[0m     new_chars \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreadsize\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1102\u001B[0m     \u001B[38;5;66;03m# If we're at a '\\r', then read one extra character, since\u001B[39;00m\n\u001B[1;32m   1103\u001B[0m     \u001B[38;5;66;03m# it might be a '\\n', to get the proper line ending.\u001B[39;00m\n\u001B[1;32m   1104\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m new_chars \u001B[38;5;129;01mand\u001B[39;00m new_chars\u001B[38;5;241m.\u001B[39mendswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m~/miniconda3/envs/msg-datathon/lib/python3.11/site-packages/nltk/data.py:1344\u001B[0m, in \u001B[0;36mSeekableUnicodeStreamReader._read\u001B[0;34m(self, size)\u001B[0m\n\u001B[1;32m   1341\u001B[0m \u001B[38;5;28mbytes\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbytebuffer \u001B[38;5;241m+\u001B[39m new_bytes\n\u001B[1;32m   1343\u001B[0m \u001B[38;5;66;03m# Decode the bytes into unicode characters\u001B[39;00m\n\u001B[0;32m-> 1344\u001B[0m chars, bytes_decoded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_incr_decode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mbytes\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1346\u001B[0m \u001B[38;5;66;03m# If we got bytes but couldn't decode any, then read further.\u001B[39;00m\n\u001B[1;32m   1347\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m chars) \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mlen\u001B[39m(new_bytes) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m):\n",
      "File \u001B[0;32m~/miniconda3/envs/msg-datathon/lib/python3.11/site-packages/nltk/data.py:1375\u001B[0m, in \u001B[0;36mSeekableUnicodeStreamReader._incr_decode\u001B[0;34m(self, bytes)\u001B[0m\n\u001B[1;32m   1373\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m   1374\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1375\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mbytes\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1376\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mUnicodeDecodeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[1;32m   1377\u001B[0m         \u001B[38;5;66;03m# If the exception occurs at the end of the string,\u001B[39;00m\n\u001B[1;32m   1378\u001B[0m         \u001B[38;5;66;03m# then assume that it's a truncation error.\u001B[39;00m\n\u001B[1;32m   1379\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m exc\u001B[38;5;241m.\u001B[39mend \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mbytes\u001B[39m):\n",
      "File \u001B[0;32m~/miniconda3/envs/msg-datathon/lib/python3.11/encodings/utf_8.py:16\u001B[0m, in \u001B[0;36mdecode\u001B[0;34m(input, errors)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode\u001B[39m(\u001B[38;5;28minput\u001B[39m, errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstrict\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m---> 16\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m codecs\u001B[38;5;241m.\u001B[39mutf_8_decode(\u001B[38;5;28minput\u001B[39m, errors, \u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "classifications = text_classifier(df_prepro_data['title'], key_features_normalized)\n",
    "classifications['date'] = df_prepro_data['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T10:27:52.655118Z",
     "start_time": "2023-11-03T10:27:52.654634Z"
    }
   },
>>>>>>> 7993297b30a61b2506f9afb5d38f110cce33df25
   "outputs": [],
   "source": [
    "# create dataset with classifications\n",
    "classifications = text_classifier(df_prepro_data['title'], key_features_normalized)\n",
    "classifications['date'] = df_prepro_data['date']\n",
    "classifications.to_csv('classification_wordnet.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "conda-env-msg-datathon-py",
   "language": "python",
   "display_name": "Python [conda env:msg-datathon]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
