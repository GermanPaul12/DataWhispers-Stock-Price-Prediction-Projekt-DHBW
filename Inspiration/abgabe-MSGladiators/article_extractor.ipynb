{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T07:11:24.154857Z",
     "start_time": "2023-11-03T07:11:22.891638Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from lxml import html\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T07:11:24.160975Z",
     "start_time": "2023-11-03T07:11:24.156670Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Datum für 43770 Tage seit 1904 ist: 2019-11-01\n"
     ]
    }
   ],
   "source": [
    "def days_to_date(days_since_1900):\n",
    "    \"\"\"\n",
    "    Funktion, um die Anzahl der Tage seit 1900 in ein Datum umzuwandeln.\n",
    "    \n",
    "    Args:\n",
    "    days_since_1900 (int): Anzahl der Tage seit dem 1. Januar 1900.\n",
    "    \n",
    "    Returns:\n",
    "    datetime.date: Das umgewandelte Datum.\n",
    "    \"\"\"\n",
    "    if isinstance(days_since_1900, str):\n",
    "        days_since_1900 = int(days_since_1900)\n",
    "    base_date = datetime(1900, 1, 1)\n",
    "    delta = timedelta(days=days_since_1900-2)\n",
    "    target_date = base_date + delta\n",
    "    return target_date\n",
    "\n",
    "# Beispielaufruf der Funktion mit der Anzahl der Tage seit 1900\n",
    "days_since_1900 = 43770  # Beispielwert\n",
    "converted_date = days_to_date(days_since_1900)\n",
    "print(f'Das Datum für {days_since_1900} Tage seit 1900 ist: {converted_date.strftime(\"%Y-%m-%d\")}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T07:11:24.162404Z",
     "start_time": "2023-11-03T07:11:24.161179Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_valid_article(tree) -> bool:\n",
    "    # Den Titel der Webseite auswählen (XPath-Ausdruck)\n",
    "    title_element = tree.xpath('//title')\n",
    "\n",
    "    # Den Textinhalt des Titel-Elements überprüfen\n",
    "    return title_element[0].text != 'Access Denied'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T07:11:24.166732Z",
     "start_time": "2023-11-03T07:11:24.163875Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_features_from_article(file_path:str) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Funktion, die Beautiful Soup und XPath verwendet, um Elemente aus einer HTML-Datei zu extrahieren.\n",
    "    \n",
    "    Args:\n",
    "    file_path (str): Der Pfad zur HTML-Datei.\n",
    "    \n",
    "    Returns:\n",
    "    list: Eine Liste mit den extrahierten Elementen.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # HTML-Datei öffnen und als BeautifulSoup-Objekt parsen\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        tree = html.fromstring(content)\n",
    "\n",
    "    if not is_valid_article(tree):\n",
    "        raise ValueError('Not a valid article. \"Access Denied\"')\n",
    "\n",
    "    title = tree.xpath('/html/body/main/div[11]/div/div/div[1]/h1')[0].text_content()\n",
    "    content = tree.xpath('/html/body/main/div[11]/div/div/div[3]/div/article/div[2]')[0].text_content()\n",
    "    \n",
    "    if content == '' or title == '':\n",
    "        raise ValueError('Not a valid article. \"Title or content is missing\"')\n",
    "\n",
    "    return title, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T07:11:24.204545Z",
     "start_time": "2023-11-03T07:11:24.203327Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import concurrent.futures\n",
    "\n",
    "directory_path = './data/'\n",
    "\n",
    "def process_subfolder(subfolder):\n",
    "    try:\n",
    "        date = days_to_date(subfolder)\n",
    "        files = [f.name for f in os.scandir(os.path.join(directory_path, subfolder)) if f.is_file()]\n",
    "        processed_articles, failed_articles = 0, 0\n",
    "\n",
    "        for file in files:\n",
    "            try:\n",
    "                title, content = extract_features_from_article(os.path.join(directory_path, subfolder, file))\n",
    "                url = ''\n",
    "                data.append((date, url, title, content))\n",
    "            except Exception as e:\n",
    "                failed_articles += 1\n",
    "                # print(f\"Failed to load file {file}. {e}\")\n",
    "            else:\n",
    "                # print(f\"Succeeded to load file {file}\")\n",
    "                processed_articles += 1\n",
    "\n",
    "        print(f\"Date: {date.strftime('%Y-%m-%d')}, Processed articles: {processed_articles}, Failed articles: {failed_articles}\")\n",
    "        return processed_articles, failed_articles\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing subfolder {subfolder}: {e}\")\n",
    "        return 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T07:12:23.650643Z",
     "start_time": "2023-11-03T07:11:24.205980Z"
    }
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "subfolders = [f.name for f in os.scandir(directory_path) if f.is_dir()]\n",
    "processed_articles = 0\n",
    "failed_articels = 0\n",
    "    \n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:  # Verwende ThreadPoolExecutor für parallele Ausführung\n",
    "    futures = [executor.submit(process_subfolder, subfolder) for subfolder in subfolders]\n",
    "\n",
    "    processed_articles, failed_articles = 0, 0\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        processed, failed = future.result()\n",
    "        processed_articles += processed\n",
    "        failed_articles += failed\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"date\", \"url\", \"title\", \"content\"])\n",
    "df.to_csv('preprocessed_data.csv')\n",
    "\n",
    "print('Dataframe saved')\n",
    "print(f\"Processed articles: {processed_articles}, Failed articles: {failed_articles}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
