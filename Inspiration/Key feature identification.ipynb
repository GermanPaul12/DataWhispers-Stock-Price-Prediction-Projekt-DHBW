{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/german/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/german/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/german/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from collections import deque\n",
    "from typing import List, Union, Dict, Set, Tuple, Sequence\n",
    "\n",
    "# nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus.reader.wordnet import Synset\n",
    "from nltk import pos_tag\n",
    "# from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df_prepro_data = pd.read_csv('preprocessed_data.csv')\n",
    "df_prepro_data = df_prepro_data[df_prepro_data['title'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Stemming\n",
    "    # stemmer = PorterStemmer()\n",
    "\n",
    "    # Removing stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "    # Normalization (converting to lower case and removing punctuation)\n",
    "    normalized_tokens = [re.sub(r'[^\\w\\s]', '', word.lower()) for word in filtered_tokens]\n",
    "    normalized_tokens = [word for word in normalized_tokens if word]  # Remove empty strings\n",
    "    normalized_tokens = [word for word in normalized_tokens if not word.isdigit()] # Remove numbers\n",
    "    normalized_tokens = [word for word in normalized_tokens if not len(word) == 1] # Remove single characters\n",
    "    normalized_tokens = [word for word in normalized_tokens if pos_tag([word])[0][1] in ['NN', 'NNS', 'NNP', 'NNPS']] # Remove non-nouns\n",
    "    normalized_tokens = [word for word in normalized_tokens if word not in string.punctuation] # Remove punctuation\n",
    "    normalized_tokens = list(filter(lambda x: not re.search(r'\\d', x), normalized_tokens)) # Remove tokens with numbers\n",
    "    # normalized_tokens = list({stemmer.stem(token) for token in normalized_tokens}) # Stemming\n",
    "\n",
    "    return normalized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build-up functions that calculate the similarity between two words \n",
    "\n",
    "def shortest_paths_to(start_syn: Synset) -> Dict[Synset, int]:\n",
    "    \"\"\"Compute the shortest distance to all nodes on paths to the root.\n",
    "    :param start_syn: synset to which we want to compute the shortest distances\n",
    "    :return: dict that matches all visited hypernyms to their distance to the input synset  \n",
    "    \"\"\" \n",
    "    # create set that keeps track of visited nodes\n",
    "    visited = set()\n",
    "    # create list which stores nodes (including distances) that need to be processed\n",
    "    queue = [(start_syn, 0)]\n",
    "    # create result dicitionary\n",
    "    distances = {}\n",
    "\n",
    "    # check nodes in the list queue as long it is not empty\n",
    "    while len(queue) > 0:\n",
    "        # remove nodes from front of list\n",
    "        syn, dist = queue.pop(0)\n",
    "        # check unvisited synsets\n",
    "        if syn in visited:\n",
    "            continue\n",
    "        visited.add(syn)\n",
    "        distances[syn] = dist\n",
    "        # loop over all direct hypernyms in list (input synset might be an instance)\n",
    "        for hyp in syn.hypernyms() + syn.instance_hypernyms():\n",
    "            # append it to queue (with dist+1) if not already looked at it before\n",
    "            if hyp not in visited or distances[hyp] > dist + syn.path_similarity(hyp):\n",
    "                queue.append((hyp, dist + syn.path_similarity(hyp)))\n",
    "                distances[hyp] = dist + syn.path_similarity(hyp)\n",
    "            # we only want to store the shortest distances\n",
    "            elif distances[hyp] > dist + syn.path_similarity(hyp):\n",
    "                distances[hyp] = dist + syn.path_similarity(hyp)\n",
    "    return distances\n",
    "\n",
    "def merge_paths(p1: Dict[Synset, int], p2: Dict[Synset, int]) -> Dict[Synset, int]:\n",
    "    \"\"\"Merge two paths keeping the shorter distance for synsets that appear more than once.\n",
    "    :param p1: first dict that maps synsets to their shortest distances\n",
    "    :param p2: second dict that maps synsets to their shortest distances\n",
    "    :return: merged dict\n",
    "    \"\"\"\n",
    "    # create resulting dictionary\n",
    "    merged = {}\n",
    "    # loop over all keys of both dictionaries\n",
    "    for synset in set(p1.keys()) | set(p2.keys()):\n",
    "        # for synsets in p1 and p2 we keep the shorter distance\n",
    "        if synset in p1 and synset in p2:\n",
    "            merged[synset] = min(p1[synset], p2[synset])\n",
    "        # distance of p1 if synset only appears in p1\n",
    "        elif synset in p1:\n",
    "            merged[synset] = p1[synset]\n",
    "        # distance of p2 if synset only appears in p2\n",
    "        else:\n",
    "            merged[synset] = p2[synset]\n",
    "            \n",
    "    return merged\n",
    "\n",
    "def all_hypernym_paths(word: str) -> Dict[Synset, int]:\n",
    "    \"\"\"Get all hypernyms of all synsets associated with the input word and compute the shortest distance leading there.\n",
    "    :param word: input word\n",
    "    :return: dict that matches all reachable hypernyms to their shortest distance \n",
    "    \"\"\"\n",
    "    # get synsets of input word\n",
    "    synsets = wn.synsets(word)\n",
    "    # create resulting dictionary\n",
    "    distances = {}\n",
    "\n",
    "    # using our functions of tasks a) and b)\n",
    "    for synset in synsets:\n",
    "        paths = shortest_paths_to(synset)\n",
    "        distances = merge_paths(distances, paths)\n",
    "\n",
    "    return distances\n",
    "\n",
    "def get_dist(w1 : str, w2 : str) -> float:\n",
    "    \"\"\"Compute the similarity between two input words in the WordNet hierarchy tree.\n",
    "    :param w1: first input word\n",
    "    :param w2: second input word\n",
    "    :return: word similarity\n",
    "    \"\"\"\n",
    "    # use function from c)\n",
    "    hyp1 = all_hypernym_paths(w1)\n",
    "    hyp2 = all_hypernym_paths(w2)\n",
    "    # filter on hypernyms that occur in both hypernym dictionaries\n",
    "    hyps = {k : hyp1[k] for k in hyp1 if k in hyp2}\n",
    "    # define d as shortest distance\n",
    "    if len(hyps) > 0:\n",
    "        d = min(hyps.values())\n",
    "        return d\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of key features\n",
    "key_features = ['politics instability' , 'geopolitics factor' , 'currency fluctuation' ,\n",
    "                'investment demand' , 'supply and demand' , 'industry demand' , 'nature disaster']\n",
    "key_features_original = ['political instability' , 'geopolitical factors' , 'currency fluctuations' ,\n",
    "                        'investment demand' , 'supply and demand' , 'industrial demand' , 'natural disasters']\n",
    "key_features_weights = [[2/5, 3/5],[4/5, 1/5], [1/2, 1/2], [4/5, 1/5], [4/5, 1/5], [4/5, 1/5], [2/5, 3/5]]\n",
    "\n",
    "key_features_normalized = [preprocess_text(key) for key in key_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final classifier function\n",
    "def text_classifier(titles: List[str], key_features_tokens: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Classify the input title according to the input key features.\n",
    "    :param df: input list with titles\n",
    "    :param key_features: list of key features\n",
    "    :return: data frame with additional column that contains the classification\n",
    "    \"\"\"\n",
    "    df_classifier = np.zeros((len(titles), len(key_features_tokens)))\n",
    "    for i, title in enumerate(titles):\n",
    "        title_tokens = preprocess_text(title)\n",
    "        if len(title_tokens) == 0:\n",
    "            continue\n",
    "        for idx_kf, key_feature in enumerate(key_features_tokens):\n",
    "            avg_distance_tt_kf = 0\n",
    "            for idx_tt, title_token in enumerate(title_tokens):\n",
    "\n",
    "                avg_distance_tt_kf += sum([get_dist(key_feature_splitted, title_token) * key_features_weights[idx_kf][j]\n",
    "                                           for j, key_feature_splitted in enumerate(key_feature)])/len(key_features)\n",
    "            df_classifier[i, idx_kf] = avg_distance_tt_kf / len(title_tokens)\n",
    "\n",
    "    return pd.DataFrame(data=df_classifier,  columns = key_features_original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset with classifications\n",
    "classifications = text_classifier(df_prepro_data['title'], key_features_normalized)\n",
    "classifications['date'] = df_prepro_data['date']\n",
    "classifications.to_csv('classification_wordnet.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
