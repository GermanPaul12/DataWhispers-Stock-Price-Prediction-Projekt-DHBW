{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from collections import deque\n",
    "from typing import List, Union, Dict, Set, Tuple, Sequence\n",
    "\n",
    "# nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus.reader.wordnet import Synset\n",
    "from nltk import pos_tag\n",
    "# from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df_prepro_data = pd.read_csv('preprocessed_data.csv')\n",
    "df_prepro_data = df_prepro_data[df_prepro_data['title'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Stemming\n",
    "    # stemmer = PorterStemmer()\n",
    "\n",
    "    # Removing stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "    # Normalization (converting to lower case and removing punctuation)\n",
    "    normalized_tokens = [re.sub(r'[^\\w\\s]', '', word.lower()) for word in filtered_tokens]\n",
    "    normalized_tokens = [word for word in normalized_tokens if word]  # Remove empty strings\n",
    "    normalized_tokens = [word for word in normalized_tokens if not word.isdigit()] # Remove numbers\n",
    "    normalized_tokens = [word for word in normalized_tokens if not len(word) == 1] # Remove single characters\n",
    "    normalized_tokens = [word for word in normalized_tokens if pos_tag([word])[0][1] in ['NN', 'NNS', 'NNP', 'NNPS']] # Remove non-nouns\n",
    "    normalized_tokens = [word for word in normalized_tokens if word not in string.punctuation] # Remove punctuation\n",
    "    normalized_tokens = list(filter(lambda x: not re.search(r'\\d', x), normalized_tokens)) # Remove tokens with numbers\n",
    "    # normalized_tokens = list({stemmer.stem(token) for token in normalized_tokens}) # Stemming\n",
    "\n",
    "    return normalized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build-up functions that calculate the similarity between two words \n",
    "\n",
    "def shortest_paths_to(start_syn: Synset) -> Dict[Synset, int]:\n",
    "    \"\"\"Compute the shortest distance to all nodes on paths to the root.\n",
    "    :param start_syn: synset to which we want to compute the shortest distances\n",
    "    :return: dict that matches all visited hypernyms to their distance to the input synset  \n",
    "    \"\"\" \n",
    "    # create set that keeps track of visited nodes\n",
    "    visited = set()\n",
    "    # create list which stores nodes (including distances) that need to be processed\n",
    "    queue = [(start_syn, 0)]\n",
    "    # create result dicitionary\n",
    "    distances = {}\n",
    "\n",
    "    # check nodes in the list queue as long it is not empty\n",
    "    while len(queue) > 0:\n",
    "        # remove nodes from front of list\n",
    "        syn, dist = queue.pop(0)\n",
    "        # check unvisited synsets\n",
    "        if syn in visited:\n",
    "            continue\n",
    "        visited.add(syn)\n",
    "        distances[syn] = dist\n",
    "        # loop over all direct hypernyms in list (input synset might be an instance)\n",
    "        for hyp in syn.hypernyms() + syn.instance_hypernyms():\n",
    "            # append it to queue (with dist+1) if not already looked at it before\n",
    "            if hyp not in visited or distances[hyp] > dist + syn.path_similarity(hyp):\n",
    "                queue.append((hyp, dist + syn.path_similarity(hyp)))\n",
    "                distances[hyp] = dist + syn.path_similarity(hyp)\n",
    "            # we only want to store the shortest distances\n",
    "            elif distances[hyp] > dist + syn.path_similarity(hyp):\n",
    "                distances[hyp] = dist + syn.path_similarity(hyp)\n",
    "    return distances\n",
    "\n",
    "def merge_paths(p1: Dict[Synset, int], p2: Dict[Synset, int]) -> Dict[Synset, int]:\n",
    "    \"\"\"Merge two paths keeping the shorter distance for synsets that appear more than once.\n",
    "    :param p1: first dict that maps synsets to their shortest distances\n",
    "    :param p2: second dict that maps synsets to their shortest distances\n",
    "    :return: merged dict\n",
    "    \"\"\"\n",
    "    # create resulting dictionary\n",
    "    merged = {}\n",
    "    # loop over all keys of both dictionaries\n",
    "    for synset in set(p1.keys()) | set(p2.keys()):\n",
    "        # for synsets in p1 and p2 we keep the shorter distance\n",
    "        if synset in p1 and synset in p2:\n",
    "            merged[synset] = min(p1[synset], p2[synset])\n",
    "        # distance of p1 if synset only appears in p1\n",
    "        elif synset in p1:\n",
    "            merged[synset] = p1[synset]\n",
    "        # distance of p2 if synset only appears in p2\n",
    "        else:\n",
    "            merged[synset] = p2[synset]\n",
    "            \n",
    "    return merged\n",
    "\n",
    "def all_hypernym_paths(word: str) -> Dict[Synset, int]:\n",
    "    \"\"\"Get all hypernyms of all synsets associated with the input word and compute the shortest distance leading there.\n",
    "    :param word: input word\n",
    "    :return: dict that matches all reachable hypernyms to their shortest distance \n",
    "    \"\"\"\n",
    "    # get synsets of input word\n",
    "    synsets = wn.synsets(word)\n",
    "    # create resulting dictionary\n",
    "    distances = {}\n",
    "\n",
    "    # using our functions of tasks a) and b)\n",
    "    for synset in synsets:\n",
    "        paths = shortest_paths_to(synset)\n",
    "        distances = merge_paths(distances, paths)\n",
    "\n",
    "    return distances\n",
    "\n",
    "def get_dist(w1 : str, w2 : str) -> float:\n",
    "    \"\"\"Compute the similarity between two input words in the WordNet hierarchy tree.\n",
    "    :param w1: first input word\n",
    "    :param w2: second input word\n",
    "    :return: word similarity\n",
    "    \"\"\"\n",
    "    # use function from c)\n",
    "    hyp1 = all_hypernym_paths(w1)\n",
    "    hyp2 = all_hypernym_paths(w2)\n",
    "    # filter on hypernyms that occur in both hypernym dictionaries\n",
    "    hyps = {k : hyp1[k] for k in hyp1 if k in hyp2}\n",
    "    # define d as shortest distance\n",
    "    if len(hyps) > 0:\n",
    "        d = min(hyps.values())\n",
    "        return d\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/german/nltk_data'\n    - '/opt/homebrew/opt/python@3.11/Frameworks/Python.framework/Versions/3.11/nltk_data'\n    - '/opt/homebrew/opt/python@3.11/Frameworks/Python.framework/Versions/3.11/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.11/Frameworks/Python.framework/Versions/3.11/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/german/Documents/Learning/Python3/Coding/DHBW/DataWhispers-Stock-Price-Prediction-Projekt-DHBW/Inspiration/abgabe-MSGladiators/Key feature identification.ipynb Cell 6\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/german/Documents/Learning/Python3/Coding/DHBW/DataWhispers-Stock-Price-Prediction-Projekt-DHBW/Inspiration/abgabe-MSGladiators/Key%20feature%20identification.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m key_features_original \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mpolitical instability\u001b[39m\u001b[39m'\u001b[39m , \u001b[39m'\u001b[39m\u001b[39mgeopolitical factors\u001b[39m\u001b[39m'\u001b[39m , \u001b[39m'\u001b[39m\u001b[39mcurrency fluctuations\u001b[39m\u001b[39m'\u001b[39m ,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/german/Documents/Learning/Python3/Coding/DHBW/DataWhispers-Stock-Price-Prediction-Projekt-DHBW/Inspiration/abgabe-MSGladiators/Key%20feature%20identification.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                         \u001b[39m'\u001b[39m\u001b[39minvestment demand\u001b[39m\u001b[39m'\u001b[39m , \u001b[39m'\u001b[39m\u001b[39msupply and demand\u001b[39m\u001b[39m'\u001b[39m , \u001b[39m'\u001b[39m\u001b[39mindustrial demand\u001b[39m\u001b[39m'\u001b[39m , \u001b[39m'\u001b[39m\u001b[39mnatural disasters\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/german/Documents/Learning/Python3/Coding/DHBW/DataWhispers-Stock-Price-Prediction-Projekt-DHBW/Inspiration/abgabe-MSGladiators/Key%20feature%20identification.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m key_features_weights \u001b[39m=\u001b[39m [[\u001b[39m2\u001b[39m\u001b[39m/\u001b[39m\u001b[39m5\u001b[39m, \u001b[39m3\u001b[39m\u001b[39m/\u001b[39m\u001b[39m5\u001b[39m],[\u001b[39m4\u001b[39m\u001b[39m/\u001b[39m\u001b[39m5\u001b[39m, \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m5\u001b[39m], [\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m], [\u001b[39m4\u001b[39m\u001b[39m/\u001b[39m\u001b[39m5\u001b[39m, \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m5\u001b[39m], [\u001b[39m4\u001b[39m\u001b[39m/\u001b[39m\u001b[39m5\u001b[39m, \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m5\u001b[39m], [\u001b[39m4\u001b[39m\u001b[39m/\u001b[39m\u001b[39m5\u001b[39m, \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m5\u001b[39m], [\u001b[39m2\u001b[39m\u001b[39m/\u001b[39m\u001b[39m5\u001b[39m, \u001b[39m3\u001b[39m\u001b[39m/\u001b[39m\u001b[39m5\u001b[39m]]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/german/Documents/Learning/Python3/Coding/DHBW/DataWhispers-Stock-Price-Prediction-Projekt-DHBW/Inspiration/abgabe-MSGladiators/Key%20feature%20identification.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m key_features_normalized \u001b[39m=\u001b[39m [preprocess_text(key) \u001b[39mfor\u001b[39;49;00m key \u001b[39min\u001b[39;49;00m key_features]\n",
      "\u001b[1;32m/Users/german/Documents/Learning/Python3/Coding/DHBW/DataWhispers-Stock-Price-Prediction-Projekt-DHBW/Inspiration/abgabe-MSGladiators/Key feature identification.ipynb Cell 6\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/german/Documents/Learning/Python3/Coding/DHBW/DataWhispers-Stock-Price-Prediction-Projekt-DHBW/Inspiration/abgabe-MSGladiators/Key%20feature%20identification.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m key_features_original \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mpolitical instability\u001b[39m\u001b[39m'\u001b[39m , \u001b[39m'\u001b[39m\u001b[39mgeopolitical factors\u001b[39m\u001b[39m'\u001b[39m , \u001b[39m'\u001b[39m\u001b[39mcurrency fluctuations\u001b[39m\u001b[39m'\u001b[39m ,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/german/Documents/Learning/Python3/Coding/DHBW/DataWhispers-Stock-Price-Prediction-Projekt-DHBW/Inspiration/abgabe-MSGladiators/Key%20feature%20identification.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                         \u001b[39m'\u001b[39m\u001b[39minvestment demand\u001b[39m\u001b[39m'\u001b[39m , \u001b[39m'\u001b[39m\u001b[39msupply and demand\u001b[39m\u001b[39m'\u001b[39m , \u001b[39m'\u001b[39m\u001b[39mindustrial demand\u001b[39m\u001b[39m'\u001b[39m , \u001b[39m'\u001b[39m\u001b[39mnatural disasters\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/german/Documents/Learning/Python3/Coding/DHBW/DataWhispers-Stock-Price-Prediction-Projekt-DHBW/Inspiration/abgabe-MSGladiators/Key%20feature%20identification.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m key_features_weights \u001b[39m=\u001b[39m [[\u001b[39m2\u001b[39m\u001b[39m/\u001b[39m\u001b[39m5\u001b[39m, \u001b[39m3\u001b[39m\u001b[39m/\u001b[39m\u001b[39m5\u001b[39m],[\u001b[39m4\u001b[39m\u001b[39m/\u001b[39m\u001b[39m5\u001b[39m, \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m5\u001b[39m], [\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m], [\u001b[39m4\u001b[39m\u001b[39m/\u001b[39m\u001b[39m5\u001b[39m, \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m5\u001b[39m], [\u001b[39m4\u001b[39m\u001b[39m/\u001b[39m\u001b[39m5\u001b[39m, \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m5\u001b[39m], [\u001b[39m4\u001b[39m\u001b[39m/\u001b[39m\u001b[39m5\u001b[39m, \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m5\u001b[39m], [\u001b[39m2\u001b[39m\u001b[39m/\u001b[39m\u001b[39m5\u001b[39m, \u001b[39m3\u001b[39m\u001b[39m/\u001b[39m\u001b[39m5\u001b[39m]]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/german/Documents/Learning/Python3/Coding/DHBW/DataWhispers-Stock-Price-Prediction-Projekt-DHBW/Inspiration/abgabe-MSGladiators/Key%20feature%20identification.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m key_features_normalized \u001b[39m=\u001b[39m [preprocess_text(key) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m key_features]\n",
      "\u001b[1;32m/Users/german/Documents/Learning/Python3/Coding/DHBW/DataWhispers-Stock-Price-Prediction-Projekt-DHBW/Inspiration/abgabe-MSGladiators/Key feature identification.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/german/Documents/Learning/Python3/Coding/DHBW/DataWhispers-Stock-Price-Prediction-Projekt-DHBW/Inspiration/abgabe-MSGladiators/Key%20feature%20identification.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_text\u001b[39m(text):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/german/Documents/Learning/Python3/Coding/DHBW/DataWhispers-Stock-Price-Prediction-Projekt-DHBW/Inspiration/abgabe-MSGladiators/Key%20feature%20identification.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m# Tokenization\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/german/Documents/Learning/Python3/Coding/DHBW/DataWhispers-Stock-Price-Prediction-Projekt-DHBW/Inspiration/abgabe-MSGladiators/Key%20feature%20identification.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     tokens \u001b[39m=\u001b[39m word_tokenize(text)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/german/Documents/Learning/Python3/Coding/DHBW/DataWhispers-Stock-Price-Prediction-Projekt-DHBW/Inspiration/abgabe-MSGladiators/Key%20feature%20identification.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# Stemming\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/german/Documents/Learning/Python3/Coding/DHBW/DataWhispers-Stock-Price-Prediction-Projekt-DHBW/Inspiration/abgabe-MSGladiators/Key%20feature%20identification.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# stemmer = PorterStemmer()\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/german/Documents/Learning/Python3/Coding/DHBW/DataWhispers-Stock-Price-Prediction-Projekt-DHBW/Inspiration/abgabe-MSGladiators/Key%20feature%20identification.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/german/Documents/Learning/Python3/Coding/DHBW/DataWhispers-Stock-Price-Prediction-Projekt-DHBW/Inspiration/abgabe-MSGladiators/Key%20feature%20identification.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# Removing stop words\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/german/Documents/Learning/Python3/Coding/DHBW/DataWhispers-Stock-Price-Prediction-Projekt-DHBW/Inspiration/abgabe-MSGladiators/Key%20feature%20identification.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     stop_words \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(stopwords\u001b[39m.\u001b[39mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlanguage\u001b[39m}\u001b[39;49;00m\u001b[39m.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[1;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/german/nltk_data'\n    - '/opt/homebrew/opt/python@3.11/Frameworks/Python.framework/Versions/3.11/nltk_data'\n    - '/opt/homebrew/opt/python@3.11/Frameworks/Python.framework/Versions/3.11/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.11/Frameworks/Python.framework/Versions/3.11/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# list of key features\n",
    "key_features = ['politics instability' , 'geopolitics factor' , 'currency fluctuation' ,\n",
    "                'investment demand' , 'supply and demand' , 'industry demand' , 'nature disaster']\n",
    "key_features_original = ['political instability' , 'geopolitical factors' , 'currency fluctuations' ,\n",
    "                        'investment demand' , 'supply and demand' , 'industrial demand' , 'natural disasters']\n",
    "key_features_weights = [[2/5, 3/5],[4/5, 1/5], [1/2, 1/2], [4/5, 1/5], [4/5, 1/5], [4/5, 1/5], [2/5, 3/5]]\n",
    "\n",
    "key_features_normalized = [preprocess_text(key) for key in key_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final classifier function\n",
    "def text_classifier(titles: List[str], key_features_tokens: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Classify the input title according to the input key features.\n",
    "    :param df: input list with titles\n",
    "    :param key_features: list of key features\n",
    "    :return: data frame with additional column that contains the classification\n",
    "    \"\"\"\n",
    "    df_classifier = np.zeros((len(titles), len(key_features_tokens)))\n",
    "    for i, title in enumerate(titles):\n",
    "        title_tokens = preprocess_text(title)\n",
    "        if len(title_tokens) == 0:\n",
    "            continue\n",
    "        for idx_kf, key_feature in enumerate(key_features_tokens):\n",
    "            avg_distance_tt_kf = 0\n",
    "            for idx_tt, title_token in enumerate(title_tokens):\n",
    "\n",
    "                avg_distance_tt_kf += sum([get_dist(key_feature_splitted, title_token) * key_features_weights[idx_kf][j]\n",
    "                                           for j, key_feature_splitted in enumerate(key_feature)])/len(key_features)\n",
    "            df_classifier[i, idx_kf] = avg_distance_tt_kf / len(title_tokens)\n",
    "\n",
    "    return pd.DataFrame(data=df_classifier,  columns = key_features_original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset with classifications\n",
    "classifications = text_classifier(df_prepro_data['title'], key_features_normalized)\n",
    "classifications['date'] = df_prepro_data['date']\n",
    "classifications.to_csv('classification_wordnet.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
